# Namo_Adhishtha
(It's a sign-language translator model.)

Sign language is a set of gestures that deaf people use to communicate. Unfortunately, normal people donâ€™t understand it, which creates a communication gap that needs to be filled. In this work, I present a computer vision system with neural networks architectures. The architecture is a CNN followed by a Long Short-Term Memory (LSTM) for extracting both spatial and temporal features. The two models achieved an accuracy of 90% and 72%, respectively. 

Sign language, as we know, is a way of communicating using hand gestures and movements, body language and facial expressions, instead of actually speaking. Like any other spoken languages such as English or Hindi, there are a bunch of different sign languages across the globe. As time passes technology is rapidly evolving and improving the way the world operates. Barriers for people with special needs are diminishing, as projects of the past two decades have unfolded.

I have strived to develop a software upon existing models which would further decrease the communication barrier for these people using Artificial Intelligence and Machine Learning, which could not only detect the sign languages but also speak it aloud.

